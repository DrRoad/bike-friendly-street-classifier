{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable as V\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms as trn\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.misc import imresize as imresize\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def load_labels():\n",
    "    # prepare all the labels\n",
    "    # scene category relevant\n",
    "    file_name_category = 'categories_places365.txt'\n",
    "    if not os.access(file_name_category, os.W_OK):\n",
    "        synset_url = 'https://raw.githubusercontent.com/csailvision/places365/master/categories_places365.txt'\n",
    "        os.system('wget ' + synset_url)\n",
    "    classes = list()\n",
    "    with open(file_name_category) as class_file:\n",
    "        for line in class_file:\n",
    "            classes.append(line.strip().split(' ')[0][3:])\n",
    "    classes = tuple(classes)\n",
    "\n",
    "    # indoor and outdoor relevant\n",
    "    file_name_IO = 'IO_places365.txt'\n",
    "    if not os.access(file_name_IO, os.W_OK):\n",
    "        synset_url = 'https://raw.githubusercontent.com/csailvision/places365/master/IO_places365.txt'\n",
    "        os.system('wget ' + synset_url)\n",
    "    with open(file_name_IO) as f:\n",
    "        lines = f.readlines()\n",
    "        labels_IO = []\n",
    "        for line in lines:\n",
    "            items = line.rstrip().split()\n",
    "            labels_IO.append(int(items[-1]) -1) # 0 is indoor, 1 is outdoor\n",
    "    labels_IO = np.array(labels_IO)\n",
    "\n",
    "    # scene attribute relevant\n",
    "    file_name_attribute = 'labels_sunattribute.txt'\n",
    "    if not os.access(file_name_attribute, os.W_OK):\n",
    "        synset_url = 'https://raw.githubusercontent.com/csailvision/places365/master/labels_sunattribute.txt'\n",
    "        os.system('wget ' + synset_url)\n",
    "    with open(file_name_attribute) as f:\n",
    "        lines = f.readlines()\n",
    "        labels_attribute = [item.rstrip() for item in lines]\n",
    "    file_name_W = 'W_sceneattribute_wideresnet18.npy'\n",
    "    if not os.access(file_name_W, os.W_OK):\n",
    "        synset_url = 'http://places2.csail.mit.edu/models_places365/W_sceneattribute_wideresnet18.npy'\n",
    "        os.system('wget ' + synset_url)\n",
    "    W_attribute = np.load(file_name_W)\n",
    "\n",
    "    return classes, labels_IO, labels_attribute, W_attribute\n",
    "\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(np.squeeze(output.data.cpu().numpy()))\n",
    "\n",
    "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "    # generate the class activation maps upsample to 256x256\n",
    "    size_upsample = (256, 256)\n",
    "    nc, h, w = feature_conv.shape\n",
    "    output_cam = []\n",
    "    for idx in class_idx:\n",
    "        cam = weight_softmax[class_idx].dot(feature_conv.reshape((nc, h*w)))\n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam_img = cam / np.max(cam)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(imresize(cam_img, size_upsample))\n",
    "    return output_cam\n",
    "\n",
    "def returnTF():\n",
    "# load the image transformer\n",
    "    tf = trn.Compose([\n",
    "        trn.Resize((224,224)),\n",
    "        trn.ToTensor(),\n",
    "        trn.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return tf\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    # this model has a last conv feature map as 14x14\n",
    "\n",
    "    model_file = 'wideresnet18_places365.pth.tar'\n",
    "    if not os.access(model_file, os.W_OK):\n",
    "        os.system('wget http://places2.csail.mit.edu/models_places365/' + model_file)\n",
    "        os.system('wget https://raw.githubusercontent.com/csailvision/places365/master/wideresnet.py')\n",
    "\n",
    "    import wideresnet\n",
    "    model = wideresnet.resnet18(num_classes=365)\n",
    "    checkpoint = torch.load(model_file, map_location=lambda storage, loc: storage)\n",
    "    state_dict = {str.replace(k,'module.',''): v for k,v in checkpoint['state_dict'].items()}\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "\n",
    "    # the following is deprecated, everything is migrated to python36\n",
    "\n",
    "    ## if you encounter the UnicodeDecodeError when use python3 to load the model, add the following line will fix it. Thanks to @soravux\n",
    "    #from functools import partial\n",
    "    #import pickle\n",
    "    #pickle.load = partial(pickle.load, encoding=\"latin1\")\n",
    "    #pickle.Unpickler = partial(pickle.Unpickler, encoding=\"latin1\")\n",
    "    #model = torch.load(model_file, map_location=lambda storage, loc: storage, pickle_module=pickle)\n",
    "\n",
    "    model.eval()\n",
    "    # hook the feature extractor\n",
    "    features_names = ['layer4','avgpool'] # this is the last conv layer of the resnet\n",
    "    for name in features_names:\n",
    "        model._modules.get(name).register_forward_hook(hook_feature)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, labels_IO, labels_attribute, W_attribute = load_labels()\n",
    "features_blobs = []\n",
    "model = load_model()\n",
    "tf = returnTF() # image transformer\n",
    "params = list(model.parameters())\n",
    "weight_softmax = params[-2].data.numpy()\n",
    "weight_softmax[weight_softmax<0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split images into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 13816\n",
      "examples: ['w100909309_n333807633.jpg', 'w100909310_n40597297.jpg', 'w100913172_n4036977663.jpg']\n",
      "Test images: 3454\n",
      "examples: ['w100263755_n3861736779.jpg', 'w100927056_n2043277677.jpg', 'w101644823_n1368468414.jpg']\n"
     ]
    }
   ],
   "source": [
    "def split_images():\n",
    "    \"\"\" \n",
    "    Sort image names alphabetically; every 5th image \n",
    "    will be a test image, and the rest train images.\n",
    "    \"\"\"\n",
    "    path_to_images = '../data/images/'\n",
    "    image_filenames = sorted(os.listdir(path_to_images))\n",
    "\n",
    "    image_filenames[0], image_filenames[-1]\n",
    "    train_filenames, test_filenames = [], []\n",
    "    for (i, filename) in enumerate(image_filenames):\n",
    "        \n",
    "        if filename[-4:] != '.jpg':\n",
    "            print(\"unexpected file: \", filename)\n",
    "            continue\n",
    "        \n",
    "        if i % 5 == 0: test_filenames.append(filename)\n",
    "        else: train_filenames.append(filename)\n",
    "\n",
    "    print(\"Train images:\", len(train_filenames))\n",
    "    print('examples:', train_filenames[:3])\n",
    "    print(\"Test images:\", len(test_filenames))\n",
    "    print('examples:', test_filenames[:3])\n",
    "\n",
    "    return train_filenames, test_filenames\n",
    "\n",
    "train_filenames, test_filenames = get_image_filenames()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Images to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_tensor(filename):\n",
    "    \"\"\" Given image filename, return a transformed tensor shape (1, C, H, W)\"\"\"\n",
    "    img = Image.open(filename)\n",
    "    input_img = tf(img).unsqueeze(0)\n",
    "    return input_img\n",
    "\n",
    "def get_final_tensor(filenames):\n",
    "    \"\"\" Given a list of image filenames, return a tensor shape (N, C, H, W)\"\"\"\n",
    "    tensors = tuple(image_to_tensor(filename) for filename in filenames)\n",
    "    X = torch.cat(tensors, dim=0)\n",
    "    print(X.shape)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"/Users/jsennett/Code/bike-friendly-street-classifier/data/images/\"\n",
    "X_test = get_final_tensor([image_dir + filename for filename in test_filenames])\n",
    "X_train = get_final_tensor([image_dir + filename for filename in train_filenames])\n",
    "print('X_train:', X_train.shape)\n",
    "print('X_train:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = torch.Tensor([labels.get(filename) for filename in test_filenames])\n",
    "Y_train = torch.Tensor([labels.get(filename) for filename in train_filenames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1., 1., 0., 1., 0., 1., 1., 1.])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat(tuple((img_tensor, img_tensor)), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13816"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filename = train_filenames[0]\n",
    "image_to_tensor(image_dir + test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(image_dir + test_filename)\n",
    "img_tensor = tf(img).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img2_tensor = torch.cat() img_tensor\n",
    "img2_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../scripts/\")\n",
    "from utils import get_image_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_labels = get_image_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint \n",
    "pprint(image_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {row.get('filename'):row.get('label') for row in image_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FriendlyStreetDataset(Dataset):\n",
    "    '''Friendly Street Dataset'''\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir: Path to image directory\n",
    "            \n",
    "            transform (callable): Optional transform to apply to sample\n",
    "        \"\"\"\n",
    "        \n",
    "        data = pd.read_csv(csv_file);\n",
    "        self.X = np.array(data.iloc[:, 1:]).reshape(-1, 1, 28, 28)#.astype(float);\n",
    "        self.Y = np.array(data.iloc[:, 0]);\n",
    "        \n",
    "        del data;\n",
    "        self.transform = transform;\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X);\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.X[idx];\n",
    "        label = self.Y[idx];\n",
    "        \n",
    "        if self.transform:\n",
    "            item = self.transform(item);\n",
    "        \n",
    "        return (item, label);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the test image\n",
    "img_url = 'http://places.csail.mit.edu/demo/6.jpg'\n",
    "os.system('wget %s -q -O test.jpg' % img_url)\n",
    "img = Image.open('test.jpg')\n",
    "input_img = V(tf(img).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"/Users/jsennett/Code/bike-friendly-street-classifier/data/images/\"\n",
    "image_path = dir_path + \"w302524881_n4100839885.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = V(tf(img).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_img.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
