{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable as V\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms as trn\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.misc import imresize as imresize\n",
    "from PIL import Image\n",
    "\n",
    "# Custom imports\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../Places365\")\n",
    "from run_placesCNN_unified import load_labels, load_model, returnCAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on gpu: False\n",
      "train 0 - 4392\n",
      "train 1 - 5970\n",
      "val 0 - 1541\n",
      "val 1 - 1913\n",
      "test 0 - 1454\n",
      "test 1 - 2000\n"
     ]
    }
   ],
   "source": [
    "# Data location\n",
    "datadir = '../data/images/'\n",
    "traindir = datadir + 'train/'\n",
    "validdir = datadir + 'val/'\n",
    "testdir = datadir + 'test/'\n",
    "\n",
    "# Batch size\n",
    "batch_size = 128\n",
    "\n",
    "# GPU Settings\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "print(f'Train on gpu: {train_on_gpu}')\n",
    "if train_on_gpu:\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f'{gpu_count} gpus detected.')\n",
    "    \n",
    "# Images per set\n",
    "for group in ['train', 'val', 'test']:\n",
    "    for label in [0, 1]:\n",
    "        path = datadir + group + '/' + str(label)\n",
    "        print(group, label, '-', len(os.listdir(path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformations\n",
    "image_transforms = {\n",
    "    # Train uses data augmentation\n",
    "    'train':\n",
    "        trn.Compose([\n",
    "        trn.Resize(size=256),\n",
    "        trn.RandomRotation(degrees=15),\n",
    "        trn.ColorJitter(),\n",
    "        trn.RandomHorizontalFlip(),\n",
    "        trn.CenterCrop(size=224),\n",
    "        trn.ToTensor(),\n",
    "        trn.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \n",
    "    # Validation does not use augmentation\n",
    "    'val':\n",
    "        trn.Compose([\n",
    "        trn.Resize(size=256),\n",
    "        trn.CenterCrop(size=224),\n",
    "        trn.ToTensor(),\n",
    "        trn.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \n",
    "    # Test does not use augmentation\n",
    "    'test':\n",
    "        trn.Compose([\n",
    "        trn.Resize(size=256),\n",
    "        trn.CenterCrop(size=224),\n",
    "        trn.ToTensor(),\n",
    "        trn.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'train': datasets.ImageFolder(root=traindir, transform=image_transforms['train']),\n",
    "    'val': datasets.ImageFolder(root=validdir, transform=image_transforms['val']),\n",
    "    'test': datasets.ImageFolder(root=testdir, transform=image_transforms['test'])\n",
    "}\n",
    "\n",
    "\n",
    "# Dataloader iterators\n",
    "dataloaders = {\n",
    "    'train': DataLoader(data['train'], batch_size=batch_size, shuffle=True),\n",
    "    'val': DataLoader(data['val'], batch_size=batch_size, shuffle=True),\n",
    "    'test': DataLoader(data['test'], batch_size=batch_size, shuffle=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 224, 224]) torch.Size([128]) batch dimensions\n",
      "2 classes\n"
     ]
    }
   ],
   "source": [
    "trainiter = iter(dataloaders['train'])\n",
    "features, labels = next(trainiter)\n",
    "print(features.shape, labels.shape, 'batch dimensions')\n",
    "print(len(data['train'].classes), 'classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set update criteria and optimizer - TODO: Tune which are used\n",
    "criterion = torch.nn.CrossEntropyLoss() # TODO: is 'cuda' right here?\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    \n",
    "    # Load the wideresnet model\n",
    "    import wideresnet\n",
    "    model = wideresnet.resnet18(num_classes=365)\n",
    "    \n",
    "    # Load in the pretrained weights\n",
    "    model_file = 'wideresnet18_places365.pth.tar'\n",
    "    checkpoint = torch.load(model_file, map_location=lambda storage, loc: storage)\n",
    "    state_dict = {str.replace(k,'module.',''): v for k,v in checkpoint['state_dict'].items()}\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    # Add hooks for attributes and CAM\n",
    "    features_names = ['layer4','avgpool']\n",
    "    for name in features_names:\n",
    "        model._modules.get(name).register_forward_hook(hook_feature)\n",
    "\n",
    "    # Freeze model weights\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Replace the final layer with a FC mapping to binary classes\n",
    "    n_inputs = model.fc.in_features\n",
    "    model.fc = torch.nn.Sequential(\n",
    "        torch.nn.Linear(n_inputs, 256), torch.nn.ReLU(), torch.nn.Dropout(0.2),\n",
    "        torch.nn.Linear(256, 2), torch.nn.LogSoftmax(dim=1))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11,308,354 total parameters.\n",
      "131,842 training parameters.\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'{total_params:,} total parameters.')\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'{total_trainable_params:,} training parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
